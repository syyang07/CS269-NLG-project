# GPT2
This is the implementation of Generative Pre-trained Transformer 2 (GPT2).

#### Reference
> Radford, A. et al. “Language Models are Unsupervised Multitask Learners.” (2019). [Link](http://www.persagen.com/files/misc/radford2019language.pdf).


## Introduction
The Transformer provides a mechanism based on encoder-decoders to detect input-output dependencies. At each step, the model consumes the previously generated symbols as additional input when generating the next output.

## Environment Requirement
The code has been tested running under Python 3.7. The required packages are as follows:
* transformers
* datasets
* sacrebleu

## How to run
- Firstly, change the current working directory to GPT2/
- Please refer to the instructions in the jupyter notebook for training, testing and generation.


================================
