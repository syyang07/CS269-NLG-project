# GPT2
This is the implementation of Generative Pre-trained Transformer 2 (GPT2)

#### Reference
> Radford, A. et al. “Language Models are Unsupervised Multitask Learners.” (2019). [Link](http://www.persagen.com/files/misc/radford2019language.pdf).


## Introduction
In this work, NGCF algorithm is improved by eliminating redundant components.

## Environment Requirement
The code has been tested running under Python 3.7. The required packages are as follows:
* transformers
* datasets
* sacrebleu

## How to run
- Firstly, change the current working directory to GPT2/
- Please refer to the instructions in the jupyter notebook for training, testing and generation.


================================
