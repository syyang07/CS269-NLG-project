{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('big-data').master(\"local[8]\").config(\"spark.driver.memory\", \"8g\").config('spark.sql.execution.arrow.enabled', 'true').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read the generated fake reviews and try to get a sense of how long the average review is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fake_reviews = spark.read.text('fake-reviews/generated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(length(value))|\n",
      "+------------------+\n",
      "|100.48283333333333|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length, mean\n",
    "\n",
    "tokenized_fake_reviews.agg(mean(length(tokenized_fake_reviews.value))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read some real reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reviews = spark.read.json('yelp-dataset/yelp_academic_dataset_review.json').sample(False, 0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to limit reviews to around 100 characters. This is obviously not 100% exact because the generated reviews are tokenized and the yelp review data set is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "real_reviews = real_reviews.where(length(real_reviews.text) <= 100).limit(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove new-lines and non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[\\\\r\\\\n]\", \" \"))\n",
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[^\\x00-\\x7F]+\", \" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|pDoDY-cDLyeKQgDx5...|   0|2012-04-28 09:28:54|    0|iS0ANobtW1ZfBdu36...|  5.0|Great food. Try t...|     0|0MKwtAFrhNslL-h5E...|\n",
      "|ybbcg01-j7tKJ_oLE...|   0|2015-04-01 15:20:43|    0|Sc0nxPCLfPt7KHdN3...|  2.0|Very average Toro...|     0|GqGlMZegcGp5GKM6N...|\n",
      "|YzMUZjUMcgI-NSGu4...|   0|2014-07-13 15:06:28|    0|TJjrhzZxMfmMjlGrw...|  4.0|Food was amazon, ...|     0|xvACCLMLVs1p4Q4va...|\n",
      "|EAs61Wm1O6tLjCs8t...|   0|2015-03-18 21:06:16|    0|LbRonpdNBwxWT4Pou...|  4.0|I recommend there...|     0|pcr9Gj69fZtU5hX6O...|\n",
      "|nrahyQyopCtajDqUt...|   0|2018-04-04 02:16:24|    0|5gKt71TDpn0LyQHlf...|  5.0|I have eaten here...|     0|tLKOtNVdoJnE4xk7C...|\n",
      "|XXW_OFaYQkkGOGniu...|   0|2017-11-06 19:25:27|    0|pneLSoCx3MBveUPBy...|  5.0|Amazing decor and...|     0|EDmcD-O81-Wk8m7xT...|\n",
      "|6kcjOA5J8NFgkx7Ds...|   0|2017-09-09 00:37:49|    0|3dLDMDVQ8hIIb6Z5Y...|  5.0|Everything in the...|     0|2z3Wy-0L2pjUO8Lq4...|\n",
      "|eTXYID00jGxq1vZpn...|   0|2016-05-31 02:25:45|    0|maJnX4s13-rktjbFi...|  5.0|From San Diego an...|     0|OkI3Nao2CECx3lwZM...|\n",
      "|hXzoNgpkC86K_Jfg_...|   0|2018-03-04 00:52:09|    0|5VfyhHQf9qETc-2jJ...|  3.0|Food was just OK....|     1|R_VS8Qxm1YYCxZbXs...|\n",
      "|ck3rA4xSMx4T8QLCw...|   0|2014-08-24 21:31:04|    0|027JU78lDle-W8dJD...|  5.0|Wonderful Greek c...|     0|EOtQzVEyq-PrDrU62...|\n",
      "|lUG4i-s3zNFY1f6bE...|   0|2018-03-04 02:47:23|    0|iwikcyeyNjzTtIRd6...|  5.0|Lizzie is amazing...|     0|BiN3SMtPQhGjT0q8O...|\n",
      "|6xgcHeLad-VaoTIQe...|   0|2011-04-22 02:55:40|    0|TERPyIClY_nBcKBzy...|  5.0|5 stars, no doubt...|     0|tVxeaerNjBJN6qaOd...|\n",
      "|N0apJkxIem2E8irTB...|   1|2015-03-04 23:09:40|    1|CNwkOOKC6YBsII_ih...|  5.0|Great food and se...|     2|FgEcp4jdbNS9UY-Bm...|\n",
      "|WnLhd38sH80ViWwzy...|   0|2017-07-08 20:07:32|    0|b8ZZxw-mEYyCK0rEx...|  5.0|You're missing ou...|     0|E-ZAACaiYtfN64y2z...|\n",
      "|z9oJeVmNEc3F0ToZ0...|   1|2015-08-07 18:10:15|    1|GV4Uke_HCB--XDN-k...|  5.0|I love the macaro...|     1|BnMIEN3a5NM3BTkhA...|\n",
      "|M51gw2cz_vXarNBLb...|   0|2015-03-05 02:53:09|    0|Fv-0yuoTQJthYog8F...|  3.0|I love grabbing b...|     0|qh9-7t43_5_aFs4mw...|\n",
      "|Hh4qRAOswrmBNWt3w...|   1|2013-12-29 14:35:23|    0|btQRcC3_9cdm5VKK8...|  5.0|Very nice staff. ...|     1|Rt1FObAgPUkYw0Jkf...|\n",
      "|7tdVsSDXRKtLLC-fD...|   0|2013-07-14 00:57:45|    0|q39LbKGHP1FXmH8p6...|  4.0|Best Mexican food...|     0|QTuwtRCrsLWDlMeAL...|\n",
      "|LtXy1VinKWfuLFslV...|   0|2015-07-16 22:11:30|    0|svgJpUe5W9y1rmoSh...|  4.0|Great burgers, I ...|     0|Z0BARqjDUjteRNoY-...|\n",
      "|cyVo7B55lWegIbV9k...|   2|2012-03-01 18:57:11|    1|prrG36HPsJ30-tszr...|  4.0|Great Place for d...|     5|vgjh4QE5wfXRHuPO5...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save for tokenization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reviews.select(real_reviews.text).write.format('text').save('fake-reviews/real_reviews.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the fake_review generation notebook, the regular tokenizers are not good enough, and we have used Stanford's CoreNLP package for tokenization when training our fake review generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 111869 tokens at 741788.36 tokens per second.\r\n"
     ]
    }
   ],
   "source": [
    "!cat fake-reviews/real_reviews.txt/part-*.txt | CLASSPATH=~/dev/stanford-parser-full-2018-10-17/stanford-parser.jar java edu.stanford.nlp.process.PTBTokenizer -lowerCase -preserveLines > fake-reviews/tokenized_real_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_real_reviews = spark.read.text('fake-reviews/tokenized_real_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_real_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "tokenized_real_reviews = tokenized_real_reviews.withColumn('label', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fake_reviews = tokenized_fake_reviews.withColumn('label', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = tokenized_fake_reviews.union(tokenized_real_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = reviews.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "tf = HashingTF(numFeatures=2**16, inputCol=\"tokens\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=4)\n",
    "pipeline = Pipeline(stages=[tokenizer, tf, idf])\n",
    "\n",
    "idf_model = pipeline.fit(train)\n",
    "train_df = idf_model.transform(train)\n",
    "val_df = idf_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100, labelCol='label')\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = lr_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under Curve / ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781689747598836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a good Area under Curve / ROC. This is very dangerous however since in real life fake and real reviews are imbalanced. There are fewer fake reviews than real reviews and we should thus use the f1-score or Area Under Curve / PR instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(df, p=1, n=0):\n",
    "    tp = df.where((df.label == p) & (df.prediction == p)).count()\n",
    "    tn = df.where((df.label == n) & (df.prediction == n)).count()\n",
    "    fp = df.where((df.label == n) & (df.prediction == p)).count()\n",
    "    fn = df.where((df.label == p) & (df.prediction == n)).count()\n",
    "    print('tp {}, fp {}, tn {}, fn {}'.format(tp, fp, tn, fn))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print('precision', precision)\n",
    "    print('recall', recall)\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR 0.9748998646699216\n",
      "tp 564, fp 50, tn 555, fn 28\n",
      "precision 0.9185667752442996\n",
      "recall 0.9527027027027027\n",
      "f1 0.9353233830845772\n"
     ]
    }
   ],
   "source": [
    "print('Area under PR', evaluator.evaluate(predict_test, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create an imbalanced test set, the Area under the curve for precision-recall and the f1-score drops significantly few a 'few fake reviews' / 'a lot of real reviews' scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.981659460346998\n",
      "Area under PR 0.9593409499530987\n",
      "tp 292, fp 50, tn 555, fn 10\n",
      "precision 0.8538011695906432\n",
      "recall 0.9668874172185431\n",
      "f1 0.906832298136646\n"
     ]
    }
   ],
   "source": [
    "imbalanced_val = val.where(val.label==1).sample(0.5).union(val.where(val.label==0))\n",
    "predict_imbalanced = lr_model.transform(idf_model.transform(imbalanced_val))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_imbalanced))\n",
    "print('Area under PR', evaluator.evaluate(predict_imbalanced, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_imbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9776888663407435\n",
      "Area under PR 0.9865839349502402\n",
      "tp 564, fp 24, tn 269, fn 28\n",
      "precision 0.9591836734693877\n",
      "recall 0.9527027027027027\n",
      "f1 0.9559322033898305\n"
     ]
    }
   ],
   "source": [
    "imbalanced_val = val.where(val.label==0).sample(0.5).union(val.where(val.label==1))\n",
    "predict_imbalanced = lr_model.transform(idf_model.transform(imbalanced_val))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_imbalanced))\n",
    "print('Area under PR', evaluator.evaluate(predict_imbalanced, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_imbalanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this test set isn't that large. We can try loading another 10k fake reviews which were created from context inputs the trained model has never seen. We can then try to achieve a 20:80 ratio of fake to real reviews (bear in mind we have no way of knowing whether the Yelp data set does not contain any fake reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_reviews = spark.read.json('yelp-dataset/yelp_academic_dataset_review.json').sample(False, 0.4, seed=42)\n",
    "more_reviews = more_reviews.where(length(more_reviews.text) <= 100).limit(40000)\n",
    "more_reviews = more_reviews.withColumn('text', regexp_replace(more_reviews.text, \"[\\\\r\\\\n]\", \" \"))\n",
    "more_reviews = more_reviews.withColumn('text', regexp_replace(more_reviews.text, \"[^\\x00-\\x7F]+\", \" \"))\n",
    "more_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 748407 tokens at 2358930.41 tokens per second.\r\n"
     ]
    }
   ],
   "source": [
    "more_reviews.select(more_reviews.text).write.format('text').save('fake-reviews/real_reviews_40k.txt')\n",
    "!cat fake-reviews/real_reviews_40k.txt/part-*.txt | CLASSPATH=~/dev/stanford-parser-full-2018-10-17/stanford-parser.jar java edu.stanford.nlp.process.PTBTokenizer -lowerCase -preserveLines > fake-reviews/tokenized_real_reviews_40k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_more_real = spark.read.text('fake-reviews/tokenized_real_reviews_40k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_more_fake = spark.read.text('fake-reviews/generated_10k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_more_real = tokenized_more_real.withColumn('label', lit(0))\n",
    "tokenized_more_fake = tokenized_more_fake.withColumn('label', lit(1))\n",
    "test_reviews = tokenized_more_real.union(tokenized_more_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9749909325000512\n",
      "Area under PR 0.8945515982757802\n",
      "tp 9378, fp 3099, tn 36901, fn 622\n",
      "precision 0.7516229862947824\n",
      "recall 0.9378\n",
      "f1 0.8344529963963163\n"
     ]
    }
   ],
   "source": [
    "predict_test = lr_model.transform(idf_model.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_test))\n",
    "print('Area under PR', evaluator.evaluate(predict_test, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at first glance, the F1-Score isn't all that bad, the precision is not that great however. Looking at the precision and recall, the question is whether we could live with these high misclassification rates in a production environment. What is more important: preventing as many fake reviews as possible, or letting through some fake reviews in order to not annoy users?\n",
    "\n",
    "We can also look at the macro averaged F1-Score for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp 9378, fp 3099, tn 36901, fn 622\n",
      "precision 0.7516229862947824\n",
      "recall 0.9378\n",
      "tp 36901, fp 622, tn 9378, fn 3099\n",
      "precision 0.983423500253178\n",
      "recall 0.922525\n",
      "avg f1 0.8932271689668332\n"
     ]
    }
   ],
   "source": [
    "print('avg f1', (f1_score(predict_test, 1, 0) + f1_score(predict_test, 0, 1))/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a naive decision tree with tf-idf as features, it's worse than a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol='label')\n",
    "dt_model = dt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.89140748125\n",
      "Area under PR 0.7149621761245883\n",
      "tp 9138, fp 4095, tn 35905, fn 862\n",
      "precision 0.690546361369304\n",
      "recall 0.9138\n",
      "f1 0.7866396935393623\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predict_dt = dt_model.transform(idf_model.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "print('Area under ROC', evaluator.evaluate(predict_dt))\n",
    "print('Area under PR', evaluator.evaluate(predict_dt, {evaluator.metricName: 'areaUnderPR'}))\n",
    "print('f1', f1_score(predict_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest performs roughly as well as a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', numTrees=100, maxDepth=4, maxBins=32)\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9820250950000251\n",
      "Area under PR 0.9473892458185246\n",
      "tp 9427, fp 2948, tn 37052, fn 573\n",
      "precision 0.7617777777777778\n",
      "recall 0.9427\n",
      "f1 0.8426368715083798\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predict_rf = rf_model.transform(idf_model.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "print('Area under ROC', evaluator.evaluate(predict_rf))\n",
    "print('Area under PR', evaluator.evaluate(predict_rf, {evaluator.metricName: 'areaUnderPR'}))\n",
    "print('f1', f1_score(predict_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "We can try to improve the logistic regression approach by using stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, NGram\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "tokenizer2 = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "stopwords2 = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\").setStopWords(stop_words)\n",
    "tf2 = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
    "idf2 = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=4)\n",
    "pipeline2 = Pipeline(stages=[tokenizer2, stopwords2, tf2, idf2])\n",
    "\n",
    "idf_model2 = pipeline.fit(train)\n",
    "train_df2 = idf_model.transform(train)\n",
    "val_df2 = idf_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(maxIter=100, labelCol='label')\n",
    "lr_model2 = lr2.fit(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9749909325000512\n",
      "Area under PR 0.8945515982757802\n",
      "tp 9378, fp 3099, tn 36901, fn 622\n",
      "precision 0.7516229862947824\n",
      "recall 0.9378\n",
      "f1 0.8344529963963163\n"
     ]
    }
   ],
   "source": [
    "predict_test2 = lr_model2.transform(idf_model2.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_test2))\n",
    "print('Area under PR', evaluator.evaluate(predict_test2, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this didn't seem to help. Another approach is to use NGrams, let's test a 1-gram, 2-gram and 3-gram input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, NGram, VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def ngram_pipeline():\n",
    "    tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "    ngrams1 = NGram(n=1, inputCol=\"tokens\", outputCol=\"ngrams1\")\n",
    "    ngrams2 = NGram(n=2, inputCol=\"tokens\", outputCol=\"ngrams2\")\n",
    "    ngrams3 = NGram(n=3, inputCol=\"tokens\", outputCol=\"ngrams3\")\n",
    "    tf1 = HashingTF(numFeatures=2**16, inputCol=\"ngrams1\", outputCol=\"tf1\")\n",
    "    tf2 = HashingTF(numFeatures=2**16, inputCol=\"ngrams2\", outputCol=\"tf2\")\n",
    "    tf3 = HashingTF(numFeatures=2**16, inputCol=\"ngrams3\", outputCol=\"tf3\")\n",
    "    idf1 = IDF(inputCol=\"tf2\", outputCol=\"idf1\", minDocFreq=4)\n",
    "    idf2 = IDF(inputCol=\"tf2\", outputCol=\"idf2\", minDocFreq=4)\n",
    "    idf3 = IDF(inputCol=\"tf3\", outputCol=\"idf3\", minDocFreq=4)\n",
    "    assembler = VectorAssembler(inputCols=[\"tf1\", \"tf2\", \"tf3\"],outputCol=\"features\")\n",
    "    return Pipeline(stages=[tokenizer, ngrams1, ngrams2, ngrams3, tf1, tf2, tf3, idf1, idf2, idf3, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_model = ngram_pipeline().fit(train)\n",
    "train_ngram = ngram_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ngram = LogisticRegression(maxIter=100, labelCol='label')\n",
    "lr_ngram_model = lr_ngram.fit(train_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this has very much improved our f1-score! False positives are significantly reduced in this skewed test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9960579725000438\n",
      "Area under PR 0.9887280759781427\n",
      "tp 9841, fp 1619, tn 38381, fn 159\n",
      "precision 0.8587260034904014\n",
      "recall 0.9841\n",
      "f1 0.917148182665424\n"
     ]
    }
   ],
   "source": [
    "predict_ngram = lr_ngram_model.transform(ngram_model.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_ngram))\n",
    "print('Area under PR', evaluator.evaluate(predict_ngram, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model also performs well on our smaller, balanced test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC 0.9959794505249049\n",
      "Area under PR 0.9961260978487302\n",
      "tp 582, fp 24, tn 581, fn 10\n",
      "precision 0.9603960396039604\n",
      "recall 0.9831081081081081\n",
      "f1 0.9716193656093489\n"
     ]
    }
   ],
   "source": [
    "predict_ngram_val = lr_ngram_model.transform(ngram_model.transform(val))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print('Area under ROC', evaluator.evaluate(predict_ngram_val))\n",
    "print('Area under PR', evaluator.evaluate(predict_ngram_val, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print('f1', f1_score(predict_ngram_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
