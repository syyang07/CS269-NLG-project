# BERT
This is the implementation of BERT model.

#### Reference
>  Devlin, J. et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL-HLT (2019). [Paper in arXiv](https://arxiv.org/abs/1810.04805).


## Introduction
In this work, we com bine the transformer-based model pre-trained on large corpus with Huggingface and form the BERT model.

## Environment Requirement
The code has been tested running under Python 3.7. The required packages are as follows:
* transformers
* torch
* datasets
* sacrebleu
* argparse

## How to run
- Firstly, change the current working directory to Item-CF/
```
cd BERT/
```
- Check running input
```
python train.py -h
```
- Example command
```

```


================================
